{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Pytorch MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vogliamo modellare un multi-layer perceptron utilizzando **Pytorch** per classificare il dataset MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 0.4.1 CUDA: True\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "print('Using PyTorch version:', torch.__version__, 'CUDA:', cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dati\n",
    "Vogliamo utilizzare il dataset MNIST, si può scaricare direttamente o caricarlo dai documenti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vogliamo ora estrarre le prime tre classi in modo da fare il train solo su queste.\n",
    "\n",
    "**ADB:** Come temevo, le ettichette devono essere vettori e non matrici con largezza 1. Nota bene come ho modificato sotto la costruzione delle ettichette. E.g. `torch.zeros((len(zeros),)` (senza 1 dopo la virgola)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = train_loader.dataset.train_labels\n",
    "td = train_loader.dataset.train_data\n",
    "\n",
    "zeros = td[tl == 0]\n",
    "ones = td[tl == 1]\n",
    "twos = td[tl == 2]\n",
    "\n",
    "data012 = torch.cat([zeros, ones, twos])\n",
    "labels012 = torch.cat([torch.zeros((len(zeros),)), torch.ones((len(ones),)), 2.0*torch.ones((len(twos),))])\n",
    "\n",
    "# ADB: Qui ho notato che c'e' un problema: le immagini sono int8, ma devono essere float32.\n",
    "# Faccio la conversione e divido per 255.0 (cosi' i pixel sono fra 0.0 e 1.0).\n",
    "data012 = data012.type(torch.float32) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adesso creiamo un nuovo dataset per il train in modo da vedere come reagisce il classificatore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vl = validation_loader.dataset.test_labels\n",
    "vd = validation_loader.dataset.test_data\n",
    "\n",
    "threes = vd[vl == 3]\n",
    "fours = vd[vl == 4]\n",
    "fives = vd[vl == 5]\n",
    "\n",
    "data345 = torch.cat([threes, fours, fives])\n",
    "#labels345 = torch.cat([torch.threes((len(threes), 1)), torch.fours((len(fours), 1)), 2.0*torch.fours((len(fives), 1))])\n",
    "\n",
    "# ADB: Qui ho notato che c'e' un problema: le immagini sono int8, ma devono essere float32.\n",
    "# Faccio la conversione e divido per 255.0 (cosi' i pixel sono fra 0.0 e 1.0).\n",
    "data345 = data345.type(torch.float32) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adesso possiamo utilizzare i nuovi dataset creati per train e test.\n",
    "\n",
    "**ADB**: credo che il problema sia che il DataLoader per training non ha le ettichette. Bisogna creare un `Dataset` oggetto dai Tensor che ha estratto. Vediamo..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADB: Ho modificato qui!\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Creo due Dataset dalle immagini e ettichette.\n",
    "train_ds012 = TensorDataset(data012, labels012)\n",
    "train_loader012 = torch.utils.data.DataLoader(train_ds012, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "# Qui le ettichette non importano, quindo penso che vada bene come ha fatto lei.\n",
    "validation_loader345 = torch.utils.data.DataLoader(data345, batch_size=batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADB: Osservazioni\n",
    "\n",
    "Stiamo mischiando un po' i concetti di 'train', 'test' e 'validate'. Per noi, servono:\n",
    "\n",
    "+ Dati IN DISTRIBUTION per training (il `train_loader012` va bene per questo.\n",
    "+ Dati IN DISTRIBUTION per validation (i.e. per monitorare quanto bene classifichiamo le classi IN DISTRIBUTION). NON abbiamo questi data per la validation per ora.\n",
    "+ Dati OUT OF DISTRIBUTION (OOD) per i test finali (questo abbiamo, ma abbiamo chiamato 'validation'). Non possiamo usare questi dati nella funzione `validate()` sotto, per esempio, perche' non abbiamo ettichette e perche' non sono IN DISTRIBUTION.\n",
    "\n",
    "Riassumendo: bisogna anche estrarre/filtrare immagini e ettichette da `validation_loader` con le ettichette `[0, 1, 2]` per usare come validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([32, 28, 28]) type: torch.FloatTensor\n",
      "y_train: torch.Size([32]) type: torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "# ADB: qui ho modificato perche' ora ogni batch contiene un gruppo di immagini e anche un gruppo di label.\n",
    "for (X_train, y_train) in train_loader012:\n",
    "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
    "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "proviamo a stampare le prime 10 immagini del train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x72 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pltsize=1\n",
    "plt.figure(figsize=(10*pltsize, pltsize))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_train[i, :, :].numpy().reshape(28,28), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "proviamo a stampare le prime 10 immagini del test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train: torch.Size([32, 28, 28]) type: torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "for Y_train in validation_loader345:\n",
    "    print('Y_train:', Y_train.size(), 'type:', Y_train.type())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x72 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pltsize=1\n",
    "plt.figure(figsize=(10*pltsize, pltsize))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(Y_train[i, :, :].numpy().reshape(28,28), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron\n",
    "Definiamo ora la rete neurale come una Python class.\n",
    "\n",
    "**ADB**: un problema con questa definizione della rete e' che il numero di output non e' 10, ma 3 nel nostro caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (fc1_drop): Dropout(p=0.2)\n",
      "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc2_drop): Dropout(p=0.2)\n",
      "  (fc3): Linear(in_features=128, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc1_drop = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc2_drop = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(128, 3)     # Numero di output 3.\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc1_drop(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc2_drop(x)\n",
    "        return F.log_softmax(self.fc3(x), 1)\n",
    "\n",
    "model = Net()\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo le funzioni di **train** e **test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADB: C'era un piccolo bug qui: enumerate() aggiunge l'indice di ogni elemento, restituendo\n",
    "# oggetti del tipo (i, (X, y)). Ho tolto enumerate che non serve.\n",
    "def train(epoch, log_interval=100):\n",
    "    model.train()\n",
    "    batch_idx = 0\n",
    "    while(batch_idx != len(train_loader012)):\n",
    "        for (data, target) in train_loader012:\n",
    "            target = target.type(torch.long)\n",
    "            if cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader012.dataset),\n",
    "                    100. * batch_idx / len(train_loader012), loss.item()))\n",
    "            batch_idx = batch_idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(loss_vector, accuracy_vector):\n",
    "    model.eval()\n",
    "    val_loss, correct = 0, 0\n",
    "    for data, target in validation_loader345:\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        output = model(data)\n",
    "        val_loss += F.nll_loss(output, target).item()\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    val_loss /= len(validation_loader345)\n",
    "    loss_vector.append(val_loss)\n",
    "\n",
    "    accuracy = 100. * correct / len(validation_loader345.dataset)\n",
    "    accuracy_vector.append(accuracy)\n",
    "    \n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        val_loss, correct, len(validation_loader345.dataset), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo adesso addestrare il nostro modello, ogni epoca passa attraverso tutto il dataset del train. Dopo ogni epoca valutiamo il modello attraverso il test(). L'obiettivo è verificare che il modello riconosce come 0, 1, 2 anche le immagini di 3, 4, 5 quindi la classificazione è **out of distribution**.\n",
    "\n",
    "**ADB**: Vedi sopra le osservazioni, ma qui abbiamo bisogno di dati IN DISTRIBUTION per usare in `validate` (per ora commentato)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/18623 (0%)]\tLoss: 0.001361\n",
      "Train Epoch: 1 [3200/18623 (17%)]\tLoss: 0.000545\n",
      "Train Epoch: 1 [6400/18623 (34%)]\tLoss: 0.000070\n",
      "Train Epoch: 1 [9600/18623 (52%)]\tLoss: 0.023201\n",
      "Train Epoch: 1 [12800/18623 (69%)]\tLoss: 0.000258\n",
      "Train Epoch: 1 [16000/18623 (86%)]\tLoss: 0.001494\n",
      "Train Epoch: 2 [0/18623 (0%)]\tLoss: 0.011513\n",
      "Train Epoch: 2 [3200/18623 (17%)]\tLoss: 0.000289\n",
      "Train Epoch: 2 [6400/18623 (34%)]\tLoss: 0.023688\n",
      "Train Epoch: 2 [9600/18623 (52%)]\tLoss: 0.000161\n",
      "Train Epoch: 2 [12800/18623 (69%)]\tLoss: 0.000688\n",
      "Train Epoch: 2 [16000/18623 (86%)]\tLoss: 0.000275\n",
      "Train Epoch: 3 [0/18623 (0%)]\tLoss: 0.005607\n",
      "Train Epoch: 3 [3200/18623 (17%)]\tLoss: 0.000091\n",
      "Train Epoch: 3 [6400/18623 (34%)]\tLoss: 0.005921\n",
      "Train Epoch: 3 [9600/18623 (52%)]\tLoss: 0.000958\n",
      "Train Epoch: 3 [12800/18623 (69%)]\tLoss: 0.002108\n",
      "Train Epoch: 3 [16000/18623 (86%)]\tLoss: 0.003347\n",
      "Train Epoch: 4 [0/18623 (0%)]\tLoss: 0.000692\n",
      "Train Epoch: 4 [3200/18623 (17%)]\tLoss: 0.000227\n",
      "Train Epoch: 4 [6400/18623 (34%)]\tLoss: 0.007547\n",
      "Train Epoch: 4 [9600/18623 (52%)]\tLoss: 0.002469\n",
      "Train Epoch: 4 [12800/18623 (69%)]\tLoss: 0.000262\n",
      "Train Epoch: 4 [16000/18623 (86%)]\tLoss: 0.004004\n",
      "Train Epoch: 5 [0/18623 (0%)]\tLoss: 0.000495\n",
      "Train Epoch: 5 [3200/18623 (17%)]\tLoss: 0.006174\n",
      "Train Epoch: 5 [6400/18623 (34%)]\tLoss: 0.013625\n",
      "Train Epoch: 5 [9600/18623 (52%)]\tLoss: 0.000405\n",
      "Train Epoch: 5 [12800/18623 (69%)]\tLoss: 0.034959\n",
      "Train Epoch: 5 [16000/18623 (86%)]\tLoss: 0.000137\n",
      "Train Epoch: 6 [0/18623 (0%)]\tLoss: 0.000119\n",
      "Train Epoch: 6 [3200/18623 (17%)]\tLoss: 0.010281\n",
      "Train Epoch: 6 [6400/18623 (34%)]\tLoss: 0.011341\n",
      "Train Epoch: 6 [9600/18623 (52%)]\tLoss: 0.014173\n",
      "Train Epoch: 6 [12800/18623 (69%)]\tLoss: 0.010852\n",
      "Train Epoch: 6 [16000/18623 (86%)]\tLoss: 0.000052\n",
      "Train Epoch: 7 [0/18623 (0%)]\tLoss: 0.002051\n",
      "Train Epoch: 7 [3200/18623 (17%)]\tLoss: 0.000835\n",
      "Train Epoch: 7 [6400/18623 (34%)]\tLoss: 0.000243\n",
      "Train Epoch: 7 [9600/18623 (52%)]\tLoss: 0.015030\n",
      "Train Epoch: 7 [12800/18623 (69%)]\tLoss: 0.000314\n",
      "Train Epoch: 7 [16000/18623 (86%)]\tLoss: 0.004960\n",
      "Train Epoch: 8 [0/18623 (0%)]\tLoss: 0.000119\n",
      "Train Epoch: 8 [3200/18623 (17%)]\tLoss: 0.002645\n",
      "Train Epoch: 8 [6400/18623 (34%)]\tLoss: 0.000632\n",
      "Train Epoch: 8 [9600/18623 (52%)]\tLoss: 0.002070\n",
      "Train Epoch: 8 [12800/18623 (69%)]\tLoss: 0.000077\n",
      "Train Epoch: 8 [16000/18623 (86%)]\tLoss: 0.001774\n",
      "Train Epoch: 9 [0/18623 (0%)]\tLoss: 0.003226\n",
      "Train Epoch: 9 [3200/18623 (17%)]\tLoss: 0.002771\n",
      "Train Epoch: 9 [6400/18623 (34%)]\tLoss: 0.000716\n",
      "Train Epoch: 9 [9600/18623 (52%)]\tLoss: 0.000581\n",
      "Train Epoch: 9 [12800/18623 (69%)]\tLoss: 0.000254\n",
      "Train Epoch: 9 [16000/18623 (86%)]\tLoss: 0.000911\n",
      "Train Epoch: 10 [0/18623 (0%)]\tLoss: 0.088356\n",
      "Train Epoch: 10 [3200/18623 (17%)]\tLoss: 0.000355\n",
      "Train Epoch: 10 [6400/18623 (34%)]\tLoss: 0.001996\n",
      "Train Epoch: 10 [9600/18623 (52%)]\tLoss: 0.000353\n",
      "Train Epoch: 10 [12800/18623 (69%)]\tLoss: 0.133744\n",
      "Train Epoch: 10 [16000/18623 (86%)]\tLoss: 0.000490\n",
      "CPU times: user 11 s, sys: 1.94 s, total: 12.9 s\n",
      "Wall time: 9.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 10\n",
    "\n",
    "lossv, accv = [], []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    # validate(lossv, accv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
